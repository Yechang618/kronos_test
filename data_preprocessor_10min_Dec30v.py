# -*- coding: utf-8 -*-
import pandas as pd
import numpy as np
from pathlib import Path

# ==============================
# é…ç½®
# ==============================
base_dir = Path("datasets")
# load_dir = base_dir
load_dir = Path("D:/data/datasets")  
processed_dir = base_dir / "processed"
processed_dir.mkdir(parents=True, exist_ok=True)

symbols = ["SOL", "BNB", "ZEC", "KAITO", "DOT", "ETH", "BTC", "LTC", "XRP", "ADA", "DOGE", "AVAX", "ETC", "TAO", # 13
           "CHESS", "COMP", "LINK", "TON", "AIXBT", "BCH", "ETH", "FET", "OM", "ONDO"] # 23
# symbol = "SOL"
symbol = symbols[0]
quote = "USDT"
pair = f"{symbol}{quote}"

start_date = "2025-01-01"
# end_date = "2025-04-30"
end_date = "2025-10-29"
date_range = pd.date_range(start=start_date, end=end_date, freq="D")
print(f"ğŸš€ Processing {pair} from {start_date} to {end_date}")

# ==============================
# å·¥å…·å‡½æ•°
# ==============================
def parse_timestamp_series(series):
    s = series.copy()
    if pd.api.types.is_numeric_dtype(s):
        max_val = s.max()
        if max_val > 1e17:
            unit = 'ns'
        elif max_val > 1e14:
            unit = 'us'
        elif max_val > 1e11:
            unit = 'ms'
        else:
            unit = 's'
        return pd.to_datetime(s, unit=unit)
    else:
        return pd.to_datetime(s)

def process_book_df(df, prefix):
    df_out = pd.DataFrame(index=df.index)
    # æå–ç¬¬0ã€1ã€2æ¡£çš„ä»·æ ¼å’Œæ•°é‡
    for level in range(3):
        price_col = f"bids[{level}].price"
        amount_col = f"bids[{level}].amount"
        if price_col in df.columns:
            df_out[f"{prefix}_bid{level}_price"] = df[price_col]
        if amount_col in df.columns:
            df_out[f"{prefix}_bid{level}_amount"] = df[amount_col]

        price_col = f"asks[{level}].price"
        amount_col = f"asks[{level}].amount"
        if price_col in df.columns:
            df_out[f"{prefix}_ask{level}_price"] = df[price_col]
        if amount_col in df.columns:
            df_out[f"{prefix}_ask{level}_amount"] = df[amount_col]
    return df_out

def process_trades_df(df, prefix):
    df = df.copy()
    if 'timestamp' not in df.columns:
        raise KeyError("'timestamp' column missing")

    if 'side' in df.columns:
        side_series = df['side']
    elif 'isBuyerMaker' in df.columns:
        side_series = df['isBuyerMaker'].map({True: 'sell', False: 'buy'})
    elif 'm' in df.columns:
        side_series = df['m'].map({True: 'sell', False: 'buy'})
    else:
        raise KeyError(f"Cannot determine side in {prefix} trades")

    if side_series.dtype == 'object':
        side_series = side_series.str.upper().map({'B': 'buy', 'S': 'sell', 'BUY': 'buy', 'SELL': 'sell'})
    else:
        side_series = side_series.map({1: 'buy', 0: 'sell', -1: 'sell', True: 'sell', False: 'buy'})

    df['side'] = side_series
    df = df[df['side'].isin(['buy', 'sell'])]

    if 'price' not in df.columns and 'p' in df.columns:
        df['price'] = df['p']
    if 'amount' not in df.columns and 'q' in df.columns:
        df['amount'] = df['q']

    required = ['price', 'amount', 'side', 'timestamp']
    for col in required:
        if col not in df.columns:
            raise KeyError(f"Missing {col} in {prefix} trades")

    df_agg = df.groupby(['side', pd.Grouper(key='timestamp', freq='1s')]).agg(
        price=('price', 'mean'),
        amount=('amount', 'mean')
    ).reset_index()

    df_wide = df_agg.pivot(index='timestamp', columns='side', values=['price', 'amount'])
    df_wide.columns = [f"{prefix}_{side}_{col}" for col, side in df_wide.columns]

    day = df['timestamp'].iloc[0].strftime('%Y-%m-%d')
    full_sec = pd.date_range(start=f"{day} 00:00:00", end=f"{day} 23:59:59", freq='1s')
    return df_wide.reindex(full_sec)

# ==============================
# ä¸»å¾ªç¯ï¼šæŒ‰å¤©åŠ è½½å¹¶ç”Ÿæˆç§’çº§æ•°æ®
# ==============================
valid_dfs = []
valid_dates = []

for single_date in date_range:
    date_str = single_date.strftime("%Y-%m-%d")
    print(f"\nğŸ“† Processing {date_str}...")

    patterns = {
        "book":     f"book/binance_book_snapshot_25_{date_str}_{pair}.csv.gz",
        "fbook":    f"fbook/binance-futures_book_snapshot_25_{date_str}_{pair}.csv.gz",
        "ftick":    f"ftick/binance-futures_derivative_ticker_{date_str}_{pair}.csv.gz",
        "ftrades":  f"ftrades/binance-futures_trades_{date_str}_{pair}.csv.gz",
        "trades":   f"trades/binance_trades_{date_str}_{pair}.csv.gz",
    }
    paths = {k: load_dir / v for k, v in patterns.items()}    

    if not (paths["book"].exists() and paths["fbook"].exists()):
        print(f"  âš ï¸ Skipping {date_str}: missing spot or futures book")
        continue

    full_second_index = pd.date_range(
        start=f"{date_str} 00:00:00",
        end=f"{date_str} 23:59:59",
        freq="1s"
    )
    dfs_to_merge = []

    try:
        # Spot
        df = pd.read_csv(paths["book"])
        df.index = parse_timestamp_series(df["timestamp"])
        df = df.sort_index()
        if df.index.duplicated().any():
            df = df[~df.index.duplicated(keep='last')]
        df_feat = process_book_df(df, "spot")
        df_res = df_feat.reindex(full_second_index, method='pad')
        dfs_to_merge.append(df_res)

        # Swap (futures)
        df = pd.read_csv(paths["fbook"])
        df.index = parse_timestamp_series(df["timestamp"])
        df = df.sort_index()
        if df.index.duplicated().any():
            df = df[~df.index.duplicated(keep='last')]
        df_feat = process_book_df(df, "swap")
        df_res = df_feat.reindex(full_second_index, method='pad')
        dfs_to_merge.append(df_res)

        # Ticker (optional)
        if paths["ftick"].exists():
            df = pd.read_csv(paths["ftick"])
            df.index = parse_timestamp_series(df["timestamp"])
            df = df.sort_index()
            if df.index.duplicated().any():
                df = df[~df.index.duplicated(keep='last')]
            df = df[["index_price", "mark_price", "funding_rate"]]
            df_res = df.reindex(full_second_index, method='pad')
            dfs_to_merge.append(df_res)

        # Trades
        if paths["trades"].exists():
            df = pd.read_csv(paths["trades"])
            df["timestamp"] = parse_timestamp_series(df["timestamp"])
            df_res = process_trades_df(df, "spot")
            dfs_to_merge.append(df_res)

        if paths["ftrades"].exists():
            df = pd.read_csv(paths["ftrades"])
            df["timestamp"] = parse_timestamp_series(df["timestamp"])
            df_res = process_trades_df(df, "swap")
            dfs_to_merge.append(df_res)

        df_day = pd.concat(dfs_to_merge, axis=1)
        df_day.index.name = "timestamp"

        first_valid = df_day["spot_bid0_price"].first_valid_index()
        if first_valid is not None:
            df_day = df_day.loc[first_valid:]
        else:
            print(f"  âš ï¸ No valid spot book, skipping {date_str}")
            continue

        valid_dfs.append(df_day)
        valid_dates.append(single_date)
        print(f"  âœ… {date_str} processed ({len(df_day)} seconds)")

    except Exception as e:
        print(f"  âŒ Error on {date_str}: {e}")
        continue

# ==============================
# åˆå¹¶æ‰€æœ‰æœ‰æ•ˆç§’çº§æ•°æ®
# ==============================
if not valid_dfs:
    print("âŒ No valid data processed.")
    exit()

all_df = pd.concat(valid_dfs, axis=0)
all_df.index.name = "timestamp"
print(f"\nğŸ“Š Total seconds: {len(all_df)}")

# ==============================
# è®¡ç®—æ–°å®šä¹‰çš„æŒ‡æ ‡
# ==============================
# ç¡®ä¿å‰ä¸‰æ¡£ä»·æ ¼/æ•°é‡å­˜åœ¨ï¼ˆç¼ºå¤±åˆ™è®¾ä¸º NaNï¼‰
required_cols = []
for asset in ['spot', 'swap']:
    for side in ['bid', 'ask']:
        for level in range(3):
            required_cols.append(f"{asset}_{side}{level}_price")
            required_cols.append(f"{asset}_{side}{level}_amount")

for col in required_cols:
    if col not in all_df.columns:
        all_df[col] = np.nan

# --- æ–°å®šä¹‰çš„ basis1 å’Œ basis2 (log price diff) ---
all_df['basis1'] = np.log(all_df['swap_bid0_price']) - np.log(all_df['spot_ask0_price'])
all_df['basis2'] = np.log(all_df['swap_ask0_price']) - np.log(all_df['spot_bid0_price'])

# --- æ–°å®šä¹‰çš„ Volumn (swap book imbalance) ---
swap_bid_sum3 = all_df[['swap_bid0_amount', 'swap_bid1_amount', 'swap_bid2_amount']].sum(axis=1)
swap_ask_sum3 = all_df[['swap_ask0_amount', 'swap_ask1_amount', 'swap_ask2_amount']].sum(axis=1)
# é¿å…é™¤é›¶æˆ– log(negative)
swap_bid_sum3 = swap_bid_sum3.replace(0, np.nan)
swap_ask_sum3 = swap_ask_sum3.replace(0, np.nan)
all_df['Volumn'] = np.log(swap_bid_sum3) - np.log(swap_ask_sum3)

# --- æ–°å®šä¹‰çš„ Amount (spot book imbalance) ---
spot_bid_sum3 = all_df[['spot_bid0_amount', 'spot_bid1_amount', 'spot_bid2_amount']].sum(axis=1)
spot_ask_sum3 = all_df[['spot_ask0_amount', 'spot_ask1_amount', 'spot_ask2_amount']].sum(axis=1)
spot_bid_sum3 = spot_bid_sum3.replace(0, np.nan)
spot_ask_sum3 = spot_ask_sum3.replace(0, np.nan)
all_df['Amount'] = np.log(spot_bid_sum3) - np.log(spot_ask_sum3)

# ==============================
# æŒ‰ 10 åˆ†é’Ÿé‡é‡‡æ ·ï¼Œèšåˆæ–°æŒ‡æ ‡
# ==============================
def agg_10min(subdf):
    if subdf.empty:
        return pd.Series(
            [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan],
            index=['Max', 'Min', 'Open', 'Close', 'Volumn', 'Amount']
        )
    
    # Max = basis1 çš„æœ€å¤§å€¼ï¼ˆæŒ‰ä½ æœ€åˆè¦æ±‚ï¼Œè€Œéåˆ†ä½æ•°ï¼‰
    Max = subdf['basis1'].quantile(0.95, interpolation='nearest')
    # Min = basis2 çš„æœ€å°å€¼
    Min = subdf['basis2'].quantile(0.05, interpolation='nearest')

    # Open / Close: (basis1 + basis2)/2 çš„é¦–å°¾é NaN å€¼
    mid_basis = (subdf['basis1'] + subdf['basis2']) / 2
    mid_clean = mid_basis.dropna()
    Open = mid_clean.iloc[0] if len(mid_clean) > 0 else np.nan
    Close = mid_clean.iloc[-1] if len(mid_clean) > 0 else np.nan

    # Volumn = è¯¥åˆ†é’Ÿå†… Volumn çš„å‡å€¼ï¼ˆæŒ‰åŸé€»è¾‘ï¼‰
    Volumn = subdf['Volumn'].mean()
    # Amount = è¯¥åˆ†é’Ÿå†… Amount çš„å‡å€¼ï¼ˆæ³¨æ„ï¼šç°åœ¨ Amount æ˜¯ç§’çº§ imbalanceï¼Œä¸æ˜¯äº¤æ˜“é‡ï¼‰
    Amount = subdf['Amount'].mean()

    return pd.Series({
        'Max': Max,
        'Min': Min,
        'Open': Open,
        'Close': Close,
        'Volumn': Volumn,
        'Amount': Amount
    })

print("\nâ³ Resampling to 10-minute intervals with log-based metrics...")

# å»é‡ï¼ˆé˜²ä¸‡ä¸€ï¼‰
all_df = all_df.loc[~all_df.index.duplicated(keep='first'), :]

basis_10min = all_df.resample('10min').apply(agg_10min)
basis_10min = basis_10min.dropna(how='all')

# ==============================
# âœ… å…³é”®ï¼šä¿®æ”¹ index åç§°ä¸º 'timestamps'
# ==============================
basis_10min.index.name = "timestamps"

# ==============================
# ä¿å­˜ç»“æœ
# ==============================
basis_dir = processed_dir / "basis_10min"
basis_dir.mkdir(exist_ok=True)

# ç¡®ä¿ basis_10min.index æ˜¯ DatetimeIndexï¼ˆåº”å·²æ˜¯ï¼‰
print("Index type after resample:", type(basis_10min.index))

grouped_basis = basis_10min.groupby(pd.Grouper(freq='MS'))
for month_start, month_df in grouped_basis:
    if not month_df.empty:
        year_month = month_start.strftime("%Y-%m")
        out_file = basis_dir / f"{pair}_basis_10min_{year_month}.csv.gz"
        print(f"ğŸ“ˆ Saving 10-min basis: {year_month}")
        month_df.to_csv(out_file, compression="gzip")

print(f"\nğŸ‰ Done! Processed {len(valid_dates)} days.")
print(f" â†’ 10-minute basis files saved in '{basis_dir}'")